```{r, include = FALSE}
dir("utils", full.names = T) |>
  sort(decreasing = T) |>
  lapply(source)
```


# Improved learner

In the first project we made the mistake of assuming that the claim indicator was known a priori. This is unrealistic, and we should instead have tried to impute the claim indicator. In order to capture as much information as possible about the certainty of the classification, we estimate the probability of making a claim. This means that the learner in this case will be a two step proces. First step will be a classification of the probability of making a claim given the features, and the second will be a regression of the claim amount for individuals who have made a claim. Denote these predictions by $\hat{p}_i$ and $\hat{C}_{1i}$ where i corresponds the the i'th observatiot. Our predicted claim amount is then given by $\hat{C}_i=\hat{p}_i\cdot \hat{C}_{1i}$. By default we use random forest for both the classification and the regression, since we concluded in project 1 that it was the best model for the regression task.

It was tricky to make a new mlr3 learner, but once the R6 object class has been understood, it is not an overly difficult task. Making a custom learner has numerous advantages, most importantly for project 2 being that we can use all the same tools for interpretability. The code for the learner can be seen here

```{r,include=TRUE,eval=FALSE}
make_learner<-function(classifier=lrn("classif.ranger", predict_type = "prob"),
                       regressor=po("encodeimpact") %>>% po("scale") %>>% lrn("regr.ranger") |>as_learner(), #Defining the classifier and regressor
                       add=F, #Should the learner be added as a learner in mlr3?
                       name="sequential.ranger"){ #What should we call the learner
  Sequential_ranger <- R6Class(
    classname=name,
    inherit = LearnerRegr, #Inherit the methods from other regression learners
    
    public = list(
      classif_model = NULL,
      regr_model = NULL,
      initialize = function() {
        super$initialize(
          id = "regr.custom.claim",
          feature_types = c("integer", "numeric", "factor", "ordered"),
          predict_types = c("response"),
          packages = c("ranger", "mlr3pipelines")
        )
      },
      
      importance = function() {
        if (is.null(self$model)) {
          stop("No model stored")
        }
        fn = self$model$features
        named_vector(fn, 0)
      }
    ),
    
    #We define the training and prediction methods
    private = list(
      .train = function(task) { #Defines how the training is done and saves the two trained models
        
        #Classification
        classif_data<-task$data() #Custom data with censored ClaimAmount
        classif_data$ClaimAmount<-NULL
        task_classif <- as_task_classif(classif_data,target = "ClaimInd")
        classif_rf <- classifier
        print("Training classifier")
        classif_rf$train(task_classif)
        self$classif_model <- classif_rf
        
        #Regression
        #Restrict to ClaimInd==1
        regr_data<-task$data()[which(task$data()$ClaimInd==1),]
        regr_data$ClaimInd<-NULL
        
        #Train regressor
        task_regr <- as_task_regr(regr_data,target="ClaimAmount")
        regr_rf <- regressor
        print("Training regression")
        regr_rf$train(task_regr)
        self$regr_model<- regr_rf
      },
      
      .predict = function(task){ #Specifies prediction
        #Make classification task for prediction
        classif_data<-task$data()
        classif_data$ClaimAmount<-NULL
        task_classif <- as_task_classif(classif_data, target = "ClaimInd")
        # Extract classification probabilities
        classif_predict <- self$classif_model$predict(task_classif)
        prob <- classif_predict$prob[,2]
        
        #Make regression task for prediction
        regr_data<- task$data()
        regr_data$ClaimInd<-NULL
        task_regr<-as_task_regr(regr_data, target="ClaimAmount")
        regr_predict <- self$regr_model$predict(task_regr)
        response <- regr_predict$response*prob
        
        return_object<-PredictionRegr$new(task=task,response=response)
        return(return_object)
      }
    )
  )
  if(add==T){
    mlr3::mlr_learners$add(name, Sequential_ranger)
  }
  else{
    return(Sequential_ranger)
  }
}
```

It would not make much sense to compare this new learner with the learners from project 1 since it is trained to work with much less information and as such would naturally give a higher MSE. Instead we simply plot the true claim amounts against the predicted ones.

```{r,include=FALSE}
seq_ranger <- readRDS("sequential_ranger_trained")
ranger_prediction <- readRDS("sequential_ranger_prediction")
```

```{r}
plot(ranger_prediction)+xlab("predicted claim amount")+ylab("true claim amount") #Modify this plot so Maristat and Gender can be seen
```

In this plot the gray line is the line y=x, where points on the line correspond to perfect predictions. The blue line is the best linear fit between the response and the truth. From this plot we can see that our sequential random forest generally underestimates the claim amount, but overall the predictions are not that bad.
